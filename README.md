# Text-Summerization-Facebook-Bart
it is aimed to train the BART model using the cnn-daily data set but the high number of datapoints in the dataset causes issues for the training.
The RAM in Google Colab is not sufficient to train the model on the CNN dataset which would be for legal document text summerization.
Two models are to be used as an ensemble T5 and BART. The architecture for BART is taken from hugging face. Hence the training
